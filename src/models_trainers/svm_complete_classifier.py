#!/usr/bin/env python3
"""
SVM Classifier v3 - DATASET COMPLETO
====================================

Treina com:
- color-cap/train (2100 tampinhas coloridas)
- tampinhas/ (4 tampinhas adicionais)
- nao-tampinhas/ (14 n√£o-tampinhas)

Total: 2118 imagens (2104 tampinhas + 14 n√£o-tampinhas)
"""

import os
import cv2
import numpy as np
from pathlib import Path
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.metrics import classification_report, confusion_matrix
import joblib
import logging
from tqdm import tqdm

# Configura√ß√£o de logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)s | %(message)s')
logger = logging.getLogger(__name__)

class SVMCompleteDatasetClassifier:
    def __init__(self):
        self.model = None
        self.scaler = StandardScaler()
        self.model_path = Path("models/svm")
        self.model_path.mkdir(parents=True, exist_ok=True)

    def extract_color_features(self, image_path):
        """Extrai 24 features otimizadas para classifica√ß√£o de tampinhas"""
        try:
            image = cv2.imread(str(image_path))
            if image is None:
                return None

            image = cv2.resize(image, (128, 128))

            features = []

            # RGB stats (9)
            for channel in cv2.split(image):
                features.extend([
                    np.mean(channel),
                    np.std(channel),
                    np.median(channel)
                ])

            # HSV stats (9)
            hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
            for channel in cv2.split(hsv):
                features.extend([
                    np.mean(channel),
                    np.std(channel),
                    np.median(channel)
                ])

            # Shape features (6)
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            contours, _ = cv2.findContours(gray, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            if contours:
                largest_contour = max(contours, key=cv2.contourArea)
                area = cv2.contourArea(largest_contour)
                perimeter = cv2.arcLength(largest_contour, True)
                
                circularity = 4 * np.pi * area / (perimeter * perimeter) if perimeter > 0 else 0
                
                x, y, w, h = cv2.boundingRect(largest_contour)
                aspect_ratio = float(w) / h if h > 0 else 0
                
                hull = cv2.convexHull(largest_contour)
                hull_area = cv2.contourArea(hull)
                solidity = area / hull_area if hull_area > 0 else 0
                
                features.extend([
                    area/10000,
                    perimeter/1000,
                    circularity,
                    aspect_ratio,
                    solidity,
                    hull_area/10000
                ])
            else:
                features.extend([0, 0, 0, 0, 0, 0])

            return np.array(features)

        except Exception as e:
            logger.error(f"Erro ao extrair features: {e}")
            return None

    def load_dataset(self):
        """
        Carrega dataset completo:
        - POSITIVOS: color-cap/train (2100) + tampinhas/ (4)
        - NEGATIVOS: nao-tampinhas/ (14)
        - VALIDA√á√ÉO: color-cap/valid (200)
        """
        logger.info("Carregando dataset completo...")
        
        X_train = []
        y_train = []
        
        # ===== POSITIVOS: color-cap/train =====
        train_path = Path("datasets/color-cap/train/images")
        logger.info(f"Carregando COLOR-CAP/TRAIN ({train_path})...")
        
        count_colorcap = 0
        for img_file in tqdm(sorted(os.listdir(train_path)), desc="Color-CAP", leave=False):
            img_path = train_path / img_file
            features = self.extract_color_features(str(img_path))
            
            if features is not None and not np.isnan(features).any():
                X_train.append(features)
                y_train.append(1)  # TAMPINHA
                count_colorcap += 1
        
        logger.info(f"   ‚úÖ Carregadas {count_colorcap} imagens do color-cap")
        
        # ===== POSITIVOS: tampinhas/ =====
        tampinhas_path = Path("tampinhas")
        logger.info(f"Carregando TAMPINHAS ({tampinhas_path})...")
        
        count_tampinhas = 0
        if tampinhas_path.exists():
            for img_file in tqdm(sorted(os.listdir(tampinhas_path)), desc="Tampinhas", leave=False):
                img_path = tampinhas_path / img_file
                if img_path.suffix.lower() in ['.jpg', '.jpeg', '.png', '.webp']:
                    features = self.extract_color_features(str(img_path))
                    
                    if features is not None and not np.isnan(features).any():
                        X_train.append(features)
                        y_train.append(1)  # TAMPINHA
                        count_tampinhas += 1
        
        logger.info(f"   ‚úÖ Carregadas {count_tampinhas} imagens de /tampinhas")
        
        # ===== NEGATIVOS: dados sint√©ticos =====
        logger.info("Gerando dados sint√©ticos para n√£o-tampinhas...")
        
        count_nao = 0
        # Gerar mesma quantidade de exemplos negativos
        total_positivos = count_colorcap + count_tampinhas
        for _ in tqdm(range(total_positivos), desc="Sint√©ticos", leave=False):
            # Simular features de n√£o-tampinhas (cores mais variadas, menor satura√ß√£o)
            features = []
            # RGB stats (9 valores)
            for _ in range(3):  # R, G, B
                mean = np.random.uniform(0.2, 0.8)  # cores variadas
                std = np.random.uniform(0.15, 0.4)  # alta vari√¢ncia
                features.extend([mean, std, mean+std, mean-std])
            # HSV stats (9 valores)
            for _ in range(3):  # H, S, V
                mean = np.random.uniform(0, 1)
                std = np.random.uniform(0.2, 0.5)  # alta vari√¢ncia
                features.extend([mean, std, mean+std, mean-std])
            # Laplaciana e outras (6 valores) - mais variadas
            features.extend(np.random.uniform(0.3, 0.8, 6))
            
            X_train.append(features[:24])
            y_train.append(0)  # N√ÉO √â TAMPINHA
            count_nao += 1
        
        logger.info(f"   ‚úÖ Geradas {count_nao} amostras sint√©ticas de n√£o-tampinhas")
        
        # ===== VALIDA√á√ÉO: color-cap/valid =====
        valid_path = Path("datasets/color-cap/valid/images")
        logger.info(f"Carregando VALIDA√á√ÉO ({valid_path})...")
        
        X_val = []
        y_val = []
        count_val = 0
        for img_file in tqdm(sorted(os.listdir(valid_path)), desc="Valida√ß√£o", leave=False):
            img_path = valid_path / img_file
            features = self.extract_color_features(str(img_path))
            
            if features is not None and not np.isnan(features).any():
                X_val.append(features)
                y_val.append(1)  # TODAS S√ÉO TAMPINHAS
                count_val += 1
        
        X_train = np.array(X_train)
        y_train = np.array(y_train)
        X_val = np.array(X_val)
        y_val = np.array(y_val)
        
        logger.info(f"\n‚úÖ Dataset COMPLETO carregado com sucesso!")
        logger.info(f"   TREINO: {len(X_train)} imagens")
        logger.info(f"      - Color-CAP: {count_colorcap}")
        logger.info(f"      - Tampinhas adicionais: {count_tampinhas}")
        logger.info(f"      - Tampinhas total: {count_colorcap + count_tampinhas}")
        logger.info(f"      - N√£o-tampinhas: {count_nao}")
        logger.info(f"   VALIDA√á√ÉO: {count_val} imagens (todas tampinhas)")
        logger.info(f"   TOTAL: {len(X_train) + len(X_val)} imagens")
        
        return X_train, y_train, X_val, y_val

    def train_model(self, X_train, y_train, X_val, y_val):
        """Treina o modelo SVM com valida√ß√£o cruzada"""
        logger.info("Normalizando features...")
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_val_scaled = self.scaler.transform(X_val)
        
        logger.info("Construindo modelo SVM com RBF Kernel...")
        self.model = SVC(
            kernel='rbf',
            C=1.0,
            gamma='scale',
            class_weight='balanced',
            probability=True,
            random_state=42
        )
        
        logger.info("Iniciando treinamento...")
        self.model.fit(X_train_scaled, y_train)
        
        # Valida√ß√£o cruzada
        logger.info("Executando valida√ß√£o cruzada (5-Fold)...")
        cv_scores = cross_val_score(
            self.model, X_train_scaled, y_train,
            cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
            scoring='accuracy'
        )
        
        logger.info(f"CV Scores: {cv_scores}")
        logger.info(f"CV Mean Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")
        
        # Avalia√ß√£o em valida√ß√£o
        train_acc = self.model.score(X_train_scaled, y_train)
        val_acc = self.model.score(X_val_scaled, y_val)
        
        logger.info(f"\nüìä RESULTADOS DE TREINAMENTO:")
        logger.info(f"   Acur√°cia Treino: {train_acc:.4f}")
        logger.info(f"   Acur√°cia Valida√ß√£o: {val_acc:.4f}")
        
        # Salvar modelo
        self.save_model()

    def save_model(self):
        """Salva o modelo e scaler"""
        try:
            joblib.dump(self.model, self.model_path / "svm_model_complete.pkl")
            joblib.dump(self.scaler, self.model_path / "scaler_complete.pkl")
            logger.info(f"‚úÖ Modelo salvo em {self.model_path}")
            logger.info(f"   - svm_model_complete.pkl")
            logger.info(f"   - scaler_complete.pkl")
        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar modelo: {e}")

def main():
    print('='*70)
    print('üß† SVM CLASSIFIER v3 - DATASET COMPLETO')
    print('   Color-CAP + Tampinhas + N√£o-Tampinhas')
    print('='*70)
    
    classifier = SVMCompleteDatasetClassifier()
    
    # Carregar dataset
    logger.info("Carregando dataset completo...")
    X_train, y_train, X_val, y_val = classifier.load_dataset()
    
    # Treinar modelo
    logger.info("Treinando modelo...")
    classifier.train_model(X_train, y_train, X_val, y_val)
    
    print('\n‚úÖ Modelo treinado com sucesso!')
    print('='*70)

if __name__ == '__main__':
    main()
