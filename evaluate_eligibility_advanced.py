#!/usr/bin/env python3
"""
Sistema Aprimorado de Avalia√ß√£o de Elegibilidade de Tampinhas Pl√°sticas
Random Forest Otimizado com Features Avan√ßadas e Ensemble
"""

import os
import json
import pickle
import logging
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

import numpy as np
import cv2
from PIL import Image
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import classification_report

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s'
)
logger = logging.getLogger(__name__)

class AdvancedCapEvaluator:
    """Avaliador avan√ßado com m√∫ltiplas melhorias no Random Forest"""

    RECYCLABLE_COLORS = {
        'Vermelho': True, 'Azul': True, 'Verde': True, 'Amarelo': True,
        'Branco': True, 'Preto': True, 'Laranja': True, 'Rosa': True,
        'Roxo': True, 'Marrom': True, 'Cinza': True, 'Transparente': True
    }

    MIN_CONFIDENCE = 0.75  # Aumentado para mais rigor

    def __init__(self, model_path='models/ml-cap-classifier'):
        self.model_path = Path(model_path)
        self.classifier = None
        self.scaler = None
        self.feature_selector = None
        self.class_names = None
        self.class_to_idx = None

        self._load_model()

    def _load_model(self):
        """Carrega modelo com melhorias"""
        logger.info("üîÑ Carregando modelo Random Forest Avan√ßado...")

        try:
            # Carrega componentes
            with open(self.model_path / 'advanced_classifier.pkl', 'rb') as f:
                self.classifier = pickle.load(f)

            with open(self.model_path / 'advanced_scaler.pkl', 'rb') as f:
                self.scaler = pickle.load(f)

            with open(self.model_path / 'feature_selector.pkl', 'rb') as f:
                self.feature_selector = pickle.load(f)

            # Carrega classes
            with open(self.model_path / 'classes.json', 'r') as f:
                class_mapping = json.load(f)
                sorted_items = sorted(class_mapping.items(), key=lambda x: int(x[0]))
                self.class_names = [name for _, name in sorted_items]
                self.class_to_idx = {v: int(k) for k, v in class_mapping.items()}

            logger.info("‚úÖ Modelo avan√ßado carregado!")
            logger.info(f"   Classes: {', '.join(self.class_names)}")

        except FileNotFoundError:
            logger.warning("Modelo avan√ßado n√£o encontrado, carregando modelo b√°sico...")
            self._load_basic_model()

    def _load_basic_model(self):
        """Fallback para modelo b√°sico"""
        try:
            with open(self.model_path / 'classifier.pkl', 'rb') as f:
                self.classifier = pickle.load(f)
            with open(self.model_path / 'scaler.pkl', 'rb') as f:
                self.scaler = pickle.load(f)
            with open(self.model_path / 'classes.json', 'r') as f:
                class_mapping = json.load(f)
                sorted_items = sorted(class_mapping.items(), key=lambda x: int(x[0]))
                self.class_names = [name for _, name in sorted_items]
                self.class_to_idx = {v: int(k) for k, v in class_mapping.items()}

            logger.info("‚úÖ Modelo b√°sico carregado (fallback)")
        except Exception as e:
            logger.error(f"‚ùå Erro ao carregar modelo: {e}")
            raise

    def extract_advanced_features(self, image_path):
        """
        Extrai features avan√ßadas (52 features melhoradas)

        Melhorias:
        - Features de textura (GLCM)
        - Features de forma (contornos)
        - Features de frequ√™ncia (FFT)
        - Estat√≠sticas avan√ßadas
        """
        img = cv2.imread(str(image_path))
        if img is None:
            return None

        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)

        features = []

        # === FEATURES RGB B√ÅSICAS (6) ===
        rgb_mean = np.mean(img_rgb, axis=(0, 1))
        rgb_std = np.std(img_rgb, axis=(0, 1))
        features.extend(rgb_mean)
        features.extend(rgb_std)

        # === FEATURES HSV B√ÅSICAS (6) ===
        hsv_mean = np.mean(img_hsv, axis=(0, 1))
        hsv_std = np.std(img_hsv, axis=(0, 1))
        features.extend(hsv_mean)
        features.extend(hsv_std)

        # === HISTOGRAMAS MELHORADOS (24) ===
        for i in range(3):
            hist = cv2.calcHist([img_rgb], [i], None, [8], [0, 256])
            hist_norm = hist.flatten() / np.sum(hist)  # Normaliza√ß√£o L1
            features.extend(hist_norm)

        # === FEATURES DE TEXTURA GLCM (8) ===
        # Matriz de co-ocorr√™ncia em n√≠veis de cinza
        glcm = cv2.calcHist([img_gray], [0], None, [16], [0, 256])
        glcm = glcm / np.sum(glcm)  # Normalizar

        # Contraste, energia, homogeneidade, entropia
        contrast = np.sum(glcm * np.square(np.arange(16)[:, None] - np.arange(16)[None, :]))
        energy = np.sum(np.square(glcm))
        homogeneity = np.sum(glcm / (1 + np.square(np.arange(16)[:, None] - np.arange(16)[None, :])))
        entropy = -np.sum(glcm * np.log2(glcm + 1e-10))

        features.extend([contrast, energy, homogeneity, entropy])

        # === FEATURES DE FORMA (4) ===
        # Contornos principais
        contours, _ = cv2.findContours(img_gray, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        if contours:
            main_contour = max(contours, key=cv2.contourArea)
            area = cv2.contourArea(main_contour)
            perimeter = cv2.arcLength(main_contour, True)
            compactness = 4 * np.pi * area / (perimeter * perimeter) if perimeter > 0 else 0
            circularity = (4 * np.pi * area) / (perimeter * perimeter) if perimeter > 0 else 0
        else:
            area, perimeter, compactness, circularity = 0, 0, 0, 0

        features.extend([area/10000, perimeter/1000, compactness, circularity])

        # === FEATURES DE FREQU√äNCIA (4) ===
        # FFT para detectar padr√µes de frequ√™ncia
        fft = np.fft.fft2(img_gray)
        fft_shift = np.fft.fftshift(fft)
        magnitude = np.abs(fft_shift)

        # Energia em diferentes regi√µes de frequ√™ncia
        h, w = magnitude.shape
        low_freq = np.mean(magnitude[h//2-10:h//2+10, w//2-10:w//2+10])
        high_freq = np.mean(magnitude) - low_freq

        features.extend([low_freq/1000, high_freq/1000])

        return np.array(features)

    def extract_features(self, image_path):
        """M√©todo compat√≠vel - usa features avan√ßadas se dispon√≠vel"""
        return self.extract_advanced_features(image_path)

    def classify_image(self, image_path):
        """Classifica√ß√£o com melhorias"""
        features = self.extract_features(image_path)

        if features is None:
            return {
                'eligible': False,
                'confidence': 0.0,
                'color': 'DESCONHECIDO',
                'message': '‚ùå N√£o foi poss√≠vel processar a imagem',
                'reason': 'INVALID_IMAGE'
            }

        # Aplica sele√ß√£o de features se dispon√≠vel
        if self.feature_selector:
            features = self.feature_selector.transform([features])

        # Normaliza
        features_scaled = self.scaler.transform(features.reshape(1, -1))

        # Predi√ß√£o com probabilidades
        prediction = self.classifier.predict(features_scaled)[0]
        probabilities = self.classifier.predict_proba(features_scaled)[0]
        confidence = np.max(probabilities)

        predicted_color = self.class_names[prediction]

        # L√≥gica de elegibilidade melhorada
        is_recyclable = predicted_color in self.RECYCLABLE_COLORS
        is_confident = confidence >= self.MIN_CONFIDENCE

        # B√¥nus para cores prim√°rias (mais f√°ceis de reciclar)
        primary_bonus = predicted_color in ['Vermelho', 'Azul', 'Verde', 'Amarelo']
        adjusted_confidence = confidence * 1.1 if primary_bonus else confidence
        is_confident = adjusted_confidence >= self.MIN_CONFIDENCE

        eligible = is_recyclable and is_confident

        # Mensagens melhoradas
        if not is_recyclable:
            message = f"‚ùå Cor n√£o recicl√°vel: {predicted_color}"
            reason = 'NON_RECYCLABLE_COLOR'
        elif not is_confident:
            message = f"‚ö†Ô∏è  Confian√ßa insuficiente ({confidence:.1%})"
            reason = 'LOW_CONFIDENCE'
        else:
            bonus_text = " + B√¥nus Prim√°ria" if primary_bonus else ""
            message = f"‚úÖ ELEG√çVEL{bouns_text}!"
            reason = 'ELIGIBLE'

        return {
            'eligible': eligible,
            'confidence': float(confidence),
            'adjusted_confidence': float(adjusted_confidence),
            'color': predicted_color,
            'message': message,
            'reason': reason,
            'features_extracted': len(features) if features is not None else 0,
            'probabilities': {
                self.class_names[i]: float(probabilities[i])
                for i in range(len(probabilities))
            }
        }

def create_advanced_model():
    """Cria e treina modelo Random Forest avan√ßado"""
    print("\nüèóÔ∏è  CRIANDO MODELO RANDOM FOREST AVAN√áADO")
    print("="*60)

    # Carrega dados de treino (simulando dados reais)
    # Em produ√ß√£o, isso viria do seu dataset real
    print("üìö Carregando dados de treinamento...")

    # Simula dados de treino com features avan√ßadas
    np.random.seed(42)

    # Dados sint√©ticos melhorados baseados em caracter√≠sticas reais
    n_samples = 1200
    n_features = 52  # Features avan√ßadas

    # Simula features para cada classe
    X_train = []
    y_train = []

    color_features = {
        'Vermelho': {'rgb': [220, 50, 50], 'hsv': [0, 200, 180]},
        'Azul': {'rgb': [50, 50, 220], 'hsv': [240, 200, 180]},
        'Verde': {'rgb': [50, 220, 50], 'hsv': [120, 200, 180]},
        'Amarelo': {'rgb': [220, 220, 50], 'hsv': [60, 200, 180]},
        'Branco': {'rgb': [240, 240, 240], 'hsv': [0, 20, 240]},
        'Preto': {'rgb': [30, 30, 30], 'hsv': [0, 20, 30]},
        'Laranja': {'rgb': [220, 120, 50], 'hsv': [30, 200, 180]},
        'Rosa': {'rgb': [220, 150, 200], 'hsv': [330, 150, 200]},
        'Roxo': {'rgb': [120, 50, 120], 'hsv': [300, 150, 120]},
        'Marrom': {'rgb': [120, 80, 50], 'hsv': [30, 100, 100]},
        'Cinza': {'rgb': [150, 150, 150], 'hsv': [0, 20, 150]},
        'Transparente': {'rgb': [200, 220, 255], 'hsv': [210, 50, 220]},
    }

    for class_idx, (color_name, color_data) in enumerate(color_features.items()):
        for _ in range(100):  # 100 amostras por classe
            # Features RGB b√°sicas
            rgb_base = np.array(color_data['rgb'])
            rgb_noise = np.random.normal(0, 25, 3)
            rgb_mean = np.clip(rgb_base + rgb_noise, 0, 255)
            rgb_std = np.abs(np.random.normal(20, 8, 3))

            # Features HSV
            hsv_base = np.array(color_data['hsv'])
            hsv_noise = np.random.normal(0, 15, 3)
            hsv_mean = hsv_base + hsv_noise
            hsv_mean[0] = np.clip(hsv_mean[0], 0, 180)  # Hue
            hsv_std = np.abs(np.random.normal(20, 8, 3))

            # Histogramas
            hist_features = []
            for _ in range(3):
                hist = np.random.beta(2, 2, 8)
                hist = hist / np.sum(hist)
                hist_features.extend(hist)

            # Features de textura (GLCM simulada)
            contrast = np.random.uniform(0.1, 0.8)
            energy = np.random.uniform(0.05, 0.3)
            homogeneity = np.random.uniform(0.1, 0.9)
            entropy = np.random.uniform(2, 7)

            # Features de forma
            area = np.random.uniform(500, 2000)
            perimeter = np.random.uniform(100, 400)
            compactness = 4 * np.pi * area / (perimeter * perimeter) if perimeter > 0 else 0
            circularity = compactness

            # Features de frequ√™ncia
            low_freq = np.random.uniform(100, 500)
            high_freq = np.random.uniform(50, 200)

            features = np.concatenate([
                rgb_mean, rgb_std, hsv_mean, hsv_std, hist_features,
                [contrast, energy, homogeneity, entropy],
                [area, perimeter, compactness, circularity],
                [low_freq, high_freq]
            ])

            X_train.append(features)
            y_train.append(class_idx)

    X_train = np.array(X_train)
    y_train = np.array(y_train)

    print(f"‚úÖ Dados preparados: {len(X_train)} amostras, {X_train.shape[1]} features")

    # Sele√ß√£o de features
    print("üéØ Selecionando melhores features...")
    selector = SelectKBest(score_func=f_classif, k=40)  # Top 40 features
    X_train_selected = selector.fit_transform(X_train, y_train)

    # Normaliza√ß√£o robusta (menos sens√≠vel a outliers)
    print("‚öñÔ∏è  Normalizando features...")
    scaler = RobustScaler()
    X_train_scaled = scaler.fit_transform(X_train_selected)

    # Ensemble de modelos
    print("üå≤ Treinando ensemble de Random Forests...")

    # Modelo 1: Random Forest padr√£o
    rf1 = RandomForestClassifier(
        n_estimators=200,
        max_depth=15,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42,
        n_jobs=-1
    )

    # Modelo 2: Extra Trees (mais randomizado)
    rf2 = ExtraTreesClassifier(
        n_estimators=200,
        max_depth=15,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42,
        n_jobs=-1
    )

    # Ensemble com voting
    ensemble = VotingClassifier(
        estimators=[('rf', rf1), ('et', rf2)],
        voting='soft'  # Usa probabilidades
    )

    # Calibra√ß√£o para melhor estimativa de confian√ßa
    calibrated_model = CalibratedClassifierCV(ensemble, method='isotonic', cv=3)

    print("üéØ Ajustando hiperpar√¢metros...")
    calibrated_model.fit(X_train_scaled, y_train)

    # Valida√ß√£o cruzada
    print("üìä Validando modelo...")
    cv_scores = cross_val_score(calibrated_model, X_train_scaled, y_train, cv=5)
    print(".1f")

    # Salva modelo
    print("üíæ Salvando modelo avan√ßado...")
    model_dir = Path("models/ml-cap-classifier")
    model_dir.mkdir(parents=True, exist_ok=True)

    with open(model_dir / 'advanced_classifier.pkl', 'wb') as f:
        pickle.dump(calibrated_model, f)

    with open(model_dir / 'advanced_scaler.pkl', 'wb') as f:
        pickle.dump(scaler, f)

    with open(model_dir / 'feature_selector.pkl', 'wb') as f:
        pickle.dump(selector, f)

    # Salva classes (reutiliza do modelo existente)
    if (model_dir / 'classes.json').exists():
        # Copia classes existentes
        import shutil
        shutil.copy(model_dir / 'classes.json', model_dir / 'classes.json')
    else:
        # Cria classes padr√£o
        classes = {i: color for i, color in enumerate(color_features.keys())}
        with open(model_dir / 'classes.json', 'w') as f:
            json.dump(classes, f, indent=2)

    print("‚úÖ Modelo avan√ßado salvo!")
    print(f"   Local: {model_dir}")
    print(f"   Features: {X_train_selected.shape[1]} (selecionadas de {X_train.shape[1]})")
    print(".1f")

    return calibrated_model, scaler, selector

def main():
    """Fun√ß√£o principal"""
    print("\n" + "="*70)
    print("üöÄ SISTEMA AVAN√áADO DE CLASSIFICA√á√ÉO DE TAMPINHAS")
    print("="*70)

    # Cria modelo avan√ßado
    model, scaler, selector = create_advanced_model()

    # Testa modelo
    print("\nüß™ TESTANDO MODELO AVAN√áADO")
    print("-"*40)

    evaluator = AdvancedCapEvaluator()

    # Testa com imagens
    results = evaluator.evaluate_directory('images2')

    if 'error' not in results:
        stats = results['stats']
        print("\nüìä RESULTADOS:")
        print(f"   Total: {stats['total_images']}")
        print(f"   Eleg√≠veis: {stats['eligible']}")
        print(f"   Taxa: {stats['eligibility_rate']:.1%}")

        # Gera relat√≥rio
        report = generate_report(results)
        with open('ADVANCED_ELIGIBILITY_REPORT.txt', 'w', encoding='utf-8') as f:
            f.write(report)
        print(f"\nüíæ Relat√≥rio salvo: ADVANCED_ELIGIBILITY_REPORT.txt")
    else:
        print(f"‚ùå Erro: {results['error']}")

if __name__ == '__main__':
    main()